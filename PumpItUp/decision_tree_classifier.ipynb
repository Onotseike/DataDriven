{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, classes : tuple, n_estimators : tuple, max_depths : tuple, train_df : pd.DataFrame, test_df : pd.DataFrame, submission_df : pd.DataFrame):\n",
    "        self.classes = classes\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depths = max_depths\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.submission_df = submission_df\n",
    "\n",
    "        self.scored_classifiers = []\n",
    "        self.classification_reports = []\n",
    "    \n",
    "    def split_dataset(self):        \n",
    "        self.train_input, self.validation_input, self.train_output, self.validation_output = tts(self.train_df.drop(columns=[\"id\", \"status_group\"]), self.train_df[\"status_group\"], test_size=0.3)\n",
    "        return self.train_input, self.validation_input, self.train_output, self.validation_output\n",
    "    \n",
    "    def create_file(self, predicited_output, file_num : str):\n",
    "        if predicited_output.shape[0] == self.submission_df.shape[0]:\n",
    "            print(True)\n",
    "            submission = self.submission_df.copy()\n",
    "            for (idx,data) in submission[\"status_group\"].iteritems():\n",
    "                submission[\"status_group\"][idx] = self.classes[predicited_output[idx]]                \n",
    "            submission.to_csv(\"./datasets/submissions/submission_\" + file_num + \".csv\", index=False, header=True)\n",
    "            del submission\n",
    "            gc.collect()\n",
    "    \n",
    "    def create_classifier(self, depth : int, estimator : int):\n",
    "        classifer_model = RandomForestClassifier(max_depth=depth, n_estimators=estimator, n_jobs=-1)\n",
    "        return classifer_model\n",
    "    \n",
    "    def train_model(self):\n",
    "        dataset = self.split_dataset()\n",
    "        for estimator in n_estimators:\n",
    "            for max_depth in max_depths:\n",
    "                classifier = self.create_classifier(depth=max_depth, estimator=estimator)\n",
    "                classifier.fit(dataset[0], dataset[2])\n",
    "                predicted_output = classifier.predict(dataset[1])\n",
    "                acc_score = accuracy_score(predicted_output, dataset[3])\n",
    "\n",
    "                self.classification_reports.append(self.create_classification_report(dataset[3], predicted_output)                )\n",
    "                self.scored_classifiers.append((estimator, max_depth, classifier, acc_score))\n",
    "    \n",
    "    def test_model(self, classifier : RandomForestClassifier,file_num : int,  create_file=True):\n",
    "        self.test_df = self.test_df.fillna(self.test_df.median())\n",
    "        predicted_output = classifier.predict(self.test_df.drop(columns=[\"id\"]))\n",
    "        if create_file:\n",
    "            self.create_file(predicted_output, str(file_num))\n",
    "        return predicted_output\n",
    "    \n",
    "    def plot_confusion_matrix(self, classifier : RandomForestClassifier):\n",
    "        plot_conf_matrix = plot_confusion_matrix(classifier, self.validation_input, self.validation_output, display_labels=classes, cmap=plt.cm.Blues, normalize='true') \n",
    "        return plot_conf_matrix\n",
    "    \n",
    "    def create_classification_report(self, validation_output, predicted_output):\n",
    "        report = classification_report(validation_output, predicted_output, output_dict=True)   \n",
    "        return pd.DataFrame(report).transpose()\n",
    "    \n",
    "    def get_cross_validation_score(self, classifier : RandomForestClassifier):\n",
    "        return cross_val_score(classifier, self.validation_input, self.validation_output, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"functional\", \"functional needs repair\",\"non functional\")\n",
    "n_estimators = (1000, 1200)\n",
    "max_depths = (40, 45, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./datasets/preprocessed_data/merged_train.csv\")\n",
    "test_df = pd.read_csv(\"./datasets/preprocessed_data/test.csv\")\n",
    "submission_df = pd.read_csv(\"./datasets/SubmissionFormat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('date_recorded', axis=1, inplace=True)\n",
    "test_df.drop('date_recorded', axis=1, inplace=True)\n",
    "#test_df.drop('status_group',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model then predict the test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(classes=classes, n_estimators=n_estimators, max_depths=max_depths, train_df=train_df, test_df=test_df, submission_df=submission_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in learner.classification_reports:\n",
    "    print(report)\n",
    "    print()\n",
    "    print(\"*\" * 90)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = learner.scored_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tuple_item in classifiers:\n",
    "    plot = learner.plot_confusion_matrix(classifier=tuple_item[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tuple_item in classifiers:\n",
    "    #plot = learner.get_cross_validation_score(classifier=tuple_item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for tuple_item in classifiers:\n",
    "    predicted_output = learner.test_model(classifier=tuple_item[2], file_num=tuple_item[0]+tuple_item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpytorchpython36conda67ab338b5aea4512a55c45a4f56020b8",
   "display_name": "Python 3.6.10 64-bit ('pytorch_python_36': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}